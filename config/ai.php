<?php

return [
    /*
    |--------------------------------------------------------------------------
    | Default AI Backend
    |--------------------------------------------------------------------------
    |
    | This option controls the default AI backend that will be used when
    | interacting with AI models. You may change this to any of the
    | backends defined in the "backends" configuration below.
    |
    */

    'default' => env('AI_BACKEND', 'ollama'),

    /*
    |--------------------------------------------------------------------------
    | Context Filter Configuration
    |--------------------------------------------------------------------------
    |
    | These options control how conversation context is filtered when
    | approaching model context limits. The default strategy and options
    | are used when an agent doesn't specify its own configuration.
    |
    */

    'context_filter' => [
        'default_strategy' => 'token_budget',
        'default_options' => [
            'budget_percentage' => 0.8,
            'reserve_tokens' => 0,
        ],
        'default_threshold' => 0.8,
    ],

    /*
    |--------------------------------------------------------------------------
    | Summarization Configuration
    |--------------------------------------------------------------------------
    |
    | These options control how conversation messages are summarized when
    | the summarization context filter strategy is used. Summaries are
    | generated by AI and stored in the database for reuse.
    |
    */

    'summarization' => [
        'enabled' => env('AI_SUMMARIZATION_ENABLED', true),
        'backend' => env('AI_SUMMARIZATION_BACKEND', null),
        'model' => env('AI_SUMMARIZATION_MODEL', null),
        'target_tokens' => 500,
        'min_messages_for_summary' => 5,
        'cache' => [
            'enabled' => true,
            'ttl' => 3600,
        ],
        'fallback_on_failure' => 'token_budget',
        'prompt' => 'Summarize the following conversation. Preserve key topics, decisions, and context. Be concise.',
    ],

    /*
    |--------------------------------------------------------------------------
    | Token Estimation Configuration
    |--------------------------------------------------------------------------
    |
    | These options control how tokens are estimated for messages.
    | Different content types have different tokenization ratios.
    |
    */

    'token_estimation' => [
        'default_chars_per_token' => 4.0,
        'json_chars_per_token' => 2.5,
        'code_chars_per_token' => 3.0,
        'safety_margin' => 0.9,
        'cache_on_message' => true,
    ],

    /*
    |--------------------------------------------------------------------------
    | AI Backend Configurations
    |--------------------------------------------------------------------------
    |
    | Here you may configure multiple AI backends. Each backend can use
    | a different driver (ollama, anthropic, openai) and has its own
    | configuration options.
    |
    */

    /*
    |--------------------------------------------------------------------------
    | RAG (Retrieval-Augmented Generation) Configuration
    |--------------------------------------------------------------------------
    |
    | These options control the RAG system including embedding generation,
    | vector storage, and retrieval strategies.
    |
    */

    'rag' => [
        'enabled' => env('RAG_ENABLED', true),

        // Embedding configuration
        'embedding_model' => env('RAG_EMBEDDING_MODEL', 'qwen3-embedding:0.6b'),
        'embedding_backend' => env('RAG_EMBEDDING_BACKEND', 'ollama'),
        'embedding_dimensions' => env('RAG_EMBEDDING_DIMENSIONS', 1536),
        'embedding_batch_size' => 100,
        'cache_embeddings' => true,

        // Search configuration
        'search_type' => env('RAG_SEARCH_TYPE', 'hybrid'), // dense, sparse, hybrid
        'top_k' => 10,
        'similarity_threshold' => 0.3,
        'max_context_tokens' => 4000,

        // Logging
        'log_retrievals' => true,
    ],

    /*
    |--------------------------------------------------------------------------
    | AI Backend Configurations
    |--------------------------------------------------------------------------
    |
    | Here you may configure multiple AI backends. Each backend can use
    | a different driver (ollama, anthropic, openai) and has its own
    | configuration options.
    |
    */

    'backends' => [
        'ollama' => [
            'driver' => 'ollama',
            'base_url' => env('OLLAMA_BASE_URL', 'http://ollama:11434'),
            'model' => env('OLLAMA_MODEL', 'llama3.1'),
            'timeout' => env('OLLAMA_TIMEOUT', 120),
            'options' => [
                'temperature' => 0.7,
                'num_ctx' => 4096,
            ],
        ],
        'vllm-gpu' => [
            'driver' => 'vllm',
            'base_url' => env('VLLM_GPU_BASE_URL', 'http://vllm-gpu:8000/v1'),
            'model' => env('VLLM_GPU_MODEL', 'meta-llama/Llama-3.1-8B-Instruct'),
            'api_key' => env('VLLM_API_KEY'),
            'max_tokens' => env('VLLM_MAX_TOKENS', 4096),
            'timeout' => env('VLLM_TIMEOUT', 120),
        ],
        'vllm-cpu' => [
            'driver' => 'vllm',
            'base_url' => env('VLLM_CPU_BASE_URL', 'http://vllm-cpu:8000/v1'),
            'model' => env('VLLM_CPU_MODEL', 'meta-llama/Llama-3.2-3B-Instruct'),
            'api_key' => env('VLLM_API_KEY'),
            'max_tokens' => env('VLLM_MAX_TOKENS', 4096),
            'timeout' => env('VLLM_TIMEOUT', 120),
        ],
        'claude' => [
            'driver' => 'anthropic',
            'api_key' => env('ANTHROPIC_API_KEY'),
            'model' => env('ANTHROPIC_MODEL', 'claude-sonnet-4-5-20250929'),
            'max_tokens' => 4096,
            'timeout' => 120,
        ],
        'openai' => [
            'driver' => 'openai',
            'api_key' => env('OPENAI_API_KEY'),
            'model' => env('OPENAI_MODEL', 'gpt-4'),
            'max_tokens' => 4096,
            'timeout' => 120,
        ],
        'huggingface' => [
            'driver' => 'huggingface',
            'api_key' => env('HUGGINGFACE_API_KEY'),
            'base_url' => env('HUGGINGFACE_BASE_URL', 'https://router.huggingface.co/v1'),
            'model' => env('HUGGINGFACE_MODEL', 'meta-llama/Llama-3.1-8B-Instruct'),
            'max_tokens' => 4096,
            'timeout' => env('HUGGINGFACE_TIMEOUT', 120),
            'provider' => env('HUGGINGFACE_PROVIDER'),
        ],
    ],
];
